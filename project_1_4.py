# -*- coding: utf-8 -*-
"""Project_1-4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1osHzPFnWsUUcU3_nwK9N8ZDv5PH2_urJ

## Spotify: Predict the top genre a song belongs to using Machine Learning Classification models.

The aim of this project is to use data from Spotify and capture various attributes of a song. The classification problem aims to predict the "top genre" that a song belongs to.We will use Python to carry out Machice Learning tasks.

The dataset is readily available from https://www.kaggle.com/datasets/cnic92/spotify-past-decades-songs-50s10s.
There are seven seperate files that are downloaded.

Using pandas we would first read the seven different files, then combine them making sure we index them correctly. The dimensions of the data is also shown (the reason for this is to see if the  sum of individuals are the same as the one final one shown). The dimension is 667 by 15.
"""

#import packages needed
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#call all the data files and put them into one whilst indexing them
data = pd.concat(
    map(pd.read_csv, ['1950.csv','1960.csv','1970.csv','1980.csv'
    ,'1990.csv','2000.csv','2010.csv']),
    ignore_index=True)

#check the dimensions
data.shape

"""We can also take a look at the last row to see the index is also coherent to the dimensions shown above. (We also checked the first and last five rows)"""

#check the indexing
#print(len(data.index))
#look at the last row
print(data.tail(1))

"""# 1. Pre-processing

Preprocessing is one of the most important step in cleaning the data before using any machine learning classification algortithms. This can be done by removing rows that have NULL/NA values. We also check each variables using a histogram.

First we look for any values that are NA or NULL, "top genre" has 16 NA values, therefore we should remove those rows.
"""

#we want to display the coloumns with NA or NULL
is_nan = data.isna().sum()
is_nan

"""We will remove the rows from the data and check again the shape of the data. The dimensions of the data is now 651 by 15"""

#drop the rows with NA
data = data.dropna()
data.shape

"""We can aslo use see the information of the data such as the column names, the null count and the data types. The data types are integers and three objects."""

#check if there are any null/na and their data types
data.info()

"""Looking at the summary statistics of the data, we see that there maybe some columns that need transformations."""

#summary statistics of the all columns
data.describe()

"""We can also look at the histograms of the the individual numerial features.

The features such as "live", "acous" and "spch" are skewed with long tails. This would make it harder for the machine learning algorithm to detect the patterns needed. The aim here is to make the shape a more bell-shaped curve. We also see that all the scales are different such as "dB" and "bpm".
"""

#check the distributions using a histograms
data.hist(bins=50, figsize=(15,10))
plt.show()

"""Lets first transform "live", "acous" and "spch". This is done by using a log transformation.

Note that the "acous" variable has many zeros, therefore we add one to it before applying the log transformation.

The "log_live" shows the bell shaped curve we are looking for. However, the "log_spch" and the "log_acous" do not show the shape needed.

We also looked at other transformations such as the square-root, cube-root and the quadratic and cubic powers with them not yielding better results.
"""

#transformation using log
#acous has a +1 because of the zeros
data["log_acous"] = np.log(data[["acous"]]+1)
data["log_spch"] = np.log(data[["spch"]])
data["log_live"] = np.log(data["live"])
#take a look at the changes
data[["log_acous","log_spch","log_live"]].hist(bins=50, figsize=(10,5))
plt.show()

"""Therefore, the variables that are not needed such as "Number", "title", "year", "live", "spch" and "acous" have been dropped. These variables are either being replaced with their transformations or they provide no insight.

The name the new dataset is "data_tr" to indicate the transformation that has occured and also a simple reference point back.
"""

#data that has been transfromed and drop features
data_tr = data.drop(["Number", "title", "artist","live",
                     "acous","year","spch"],
                    axis=1)
#check the shape
data_tr.shape

"""We should also standardise the scales, this is because the algorithms do not perform well with different scales where the "bpm" is between 6 to 199, whereas "dB" is between -24 to -1.
Using the StandardScaler from the sklearn.preprocessing package, we standardised the scales for all variables.

Note we also need to seperate the numerical and categorical data and then scale the numerical data.
"""

#import the StandardScaler
from sklearn.preprocessing import StandardScaler

#call the function
scaler = StandardScaler()

#seperate the numerical and the categorical variables
data_cat = data_tr["top genre"]
data_num = data_tr.drop(["top genre"], axis=1)

#take account of the numerical coloumns
data_num_col = data_num.columns

#scale and standardise the data
data_num = scaler.fit_transform(data_num)

#put it back into a dataframe
data_num = pd.DataFrame(data_num, columns=data_num_col)

"""A final look at the histograms, we see that most have a bell shaped curve and have a mean around 0. The "spch" is still skewed, however it is in the range of the other variables such as "dur".

Checking if there is any correlations between the variables using heatmap provided by the seaborn package and the correlation funtion. The two variables, "nrgy" and "dB" are highly postively correlated (0.70). We also see that the "log_acous" with the both the "dB" and "nrgy" are negatively corelated ("log_acous" and "dB" is -0.44, "log_acous" and "nrgy" -0.56). There is also little or no correlation between "val" and "log_acous" (0.0031), this shows that they have no linear relationship, but can have a non-linear relationship.
"""

#use correlations of the numeric feature
corr = data_num.corr()
#print correlations
print(corr)

#import seaborn package
import seaborn as sns

#plot the heatmap
sns.heatmap(corr, xticklabels=corr.columns.values,
            yticklabels=corr.columns.values)
plt.show()

"""Now that we have preprocessed the numerical variables, let us look into the categorical variables. We see that "dance pop" has the most frequent counts (114) and many have a count of 1 such as afrobeat, afropop and british blues.

Also notice that there are 115 unique categories with many having a count less than 77.

We ran the process using a logisitc regression of both the binary and the multiclass, we found that the performance of the classification was very low (accuracy of 0.2).
Therefore we reduced the "top genre" target group to only 3, "dance pop", "adult standards" and "other".

Leading to the "others" genre having 444 counts, which means the "dance pop" has 114 and the "adult standard" has 93. We also notice that the groups are unbalanced.

We seperate the transformed data into two. This will allow us to split it the data.
"""

#recall the transformed data
data_tr

#check the counts
data_tr["top genre"].value_counts()

#create a group "other" if we have less than 80 counts and rename the
#the groups which are not above 80 into other
for genre in data_tr['top genre'].unique():
    if data_tr[data_tr['top genre'] == genre].shape[0] <= 80:
        data_tr['top genre'] = data_tr['top genre'].replace(genre, 'other')

#show the genres and their counts
data_tr['top genre'].value_counts()

"""# 2. Train-Test Split

We will begin by first having a data group and a target group. The target group is the "top genre" and the data group is the numerical data.

We will split the data into a training group and a testing group, with 70%-30% split ratio and a random state where the answers can be replicated.
"""

#import the train test split model selection
from sklearn.model_selection import train_test_split

#seperate the features and target groups
X = data_tr.drop(["top genre"], axis=1)
y = data_tr["top genre"]

#split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
                                                    random_state=24)

#check the dimension
X_train.shape, X_test.shape

"""# 3. Logistic Regression

Logisitic Regression is a model that is used to estimate that particular instance belong to a specific class. This make is classification model. If a probability of an instance is greater than 0.5 then it is put in to the class "1" or it will classified in class "0". This makes it a binary classifier [1].

We will use the basic logistic regression and compare it with a multiclass approach.

The multiclass model is a One-Vs-Rest (OVR) approach, which starts with "k" classifiers and have a one class detector. This continues until we have "k" classifiers with "k" detecters [1].

We will fit the models using the "X_train" and "y_train", then check the performance of the training set which should decide if the model is a good fit.  
We will then predict the model with the "X_test" and finally evaluate the model using the predicted values with the "y_test". This will be done for all the machine learning models.

The performance is measured by the confusion matrix which is the preferred measurement for classification problems [1]. The accuracy which is the ratio of correct prediction.
The precision which is the accuracy of positive prediction, the recall which is also known as the sensitity or the ratio of postive instances correctly detected by the classifier and finally the F1 score which combines the precision and the recall thus giving us a metric to compare two classifiers [1].

The precision and the recall are derived from the confusion matrix. There is inverse relationship between the recall and the recall meaning that increasing the recall would decrease the precision and vice versa.
"""

#import the logisitic regression and the performance metrics
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score


#Fit the logistic regression
log_reg = LogisticRegression(random_state=24)
log_reg.fit(X_train, y_train)

#Evaluation with the Training set with 5 rows
print("Accuracy of training set with 5:",
      accuracy_score(y_train[:5],log_reg.predict(X_train[:5])))

#Evaluation with the Training set
print("Accuracy of training set:",
      accuracy_score(y_train,log_reg.predict(X_train)))

#predict using the x test
pred = log_reg.predict(X_test)

#Evaluation with the testing set
print("Testing set Performance" )
print("Confusion Matrix: ", "\n",
      confusion_matrix(y_test, pred))
print("Accuracy: ", accuracy_score(y_test, pred))
print("Precision: ", precision_score(y_test, pred, average="weighted"))
print("Recall: ", recall_score(y_test, pred, average="weighted"))
print("F1: ", f1_score(y_test, pred, average="weighted"))

#Confusion Matrix plot
conf_mat = confusion_matrix(y_test, pred)
plt.matshow(conf_mat, cmap= plt.cm.rainbow)
plt.title("Confusion Matrix")
plt.colorbar()
plt.show()

"""A quick look at the performance of the training set with 5 rows has a accuracy of 1.0 meaning it perfectly predicited the genre. However, when we increase the training set the accuracy decreases to 0.70. This shows that the basic logisitc regression might not be the best model. We will explore other models.

The accuarcy of the testing set of the logisitic regression is 0.69, the precision is 0.63, the recall is 0.69 and the f1 score is 0.62. This has tripled the improvement of the model classification in all the performance metrics compare to the 115 genres.

The confusion matrix plot shows that group two which is "other" is prediciting well. We also note that the confusion matrix has very contrasting colours or numbers, this is explained by the "other" genre group having 136 while "dance pop" and "adult standards" having 35 and 25 respectively.

#4. One Vs Rest Classifier
"""

#import the muticlass OVR classifier
from sklearn.multiclass import OneVsRestClassifier

#Use the logistic regression
log_reg = LogisticRegression(random_state=24)

#Use the OVR classifer on the logistic regression
ovr = OneVsRestClassifier(log_reg)

#fit the OVR classifer
ovr.fit(X_train, y_train)

#Evaluation with the Training set with 5 rows
print("Accuracy of training set with 5:",
      accuracy_score(y_train[:5],ovr.predict(X_train[:5])))

#Evaluation with the Training set
print("Accuracy of training set:",
      accuracy_score(y_train,ovr.predict(X_train)))


#predict the OVR with the X_test
pred = ovr.predict(X_test)


#Evaluation with the y_test
print("Confusion Matrix: ", "\n", confusion_matrix(y_test, pred))
print("Accuracy: ", accuracy_score(y_test, pred))
print("Precision: ", precision_score(y_test, pred, average="weighted"))
print("Recall: ", recall_score(y_test, pred, average="weighted"))
print("F1: ", f1_score(y_test, pred, average="weighted"))

#Confusion Matrix plot
conf_mat = confusion_matrix(y_test, pred)
plt.matshow(conf_mat, cmap= plt.cm.rainbow)
plt.title("Confusion Matrix")
plt.colorbar()
plt.show()

"""The performance of the training set with 5 rows has a accuracy of 1.0 meaning it also perfectly predicted the genre. However, when we increase the training set the accuracy decreases to 0.70 which is the same performance as the simple logostic regression.

The accuracy of the testing set of the OVR logistic regression is 0.71, the precision is 0.67, the recall is 0.71 and the f1 score is 0.64.

Also the a slight improvement to the simple logistic regression with three classes.

# 5. Decision Tree

Decision Trees are one of the most robost algorithms, which have the capabilties of fitting complex datasets [1]. In our case this would be a good model to fit and predict. This is done with the evaluation of performance of the training set.

One feature of the decision tree is that it make few assumptions and is quite adaptive to the training set [1]. This could cause it to overfit [1]. The overfitting is controlled by the max_depth parameter [1].
"""

#import Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier

#fit the tree classifier
tree_clf = DecisionTreeClassifier(max_depth=2, random_state=24)
tree_clf.fit(X_train, y_train)

#Evaluation with the Training set with 5 rows
print("Accuracy of training set with 5:",
      accuracy_score(y_train[:5],tree_clf.predict(X_train[:5])))

#Evaluation with the Training set
print("Accuracy of training set:",
      accuracy_score(y_train,tree_clf.predict(X_train)))

#predict using the X_test
pred = tree_clf.predict(X_test)

#Evaluation with the y_test
print("Confusion Matrix: ", "\n", confusion_matrix(y_test, pred))
print("Accuracy: ", accuracy_score(y_test, pred))
print("Precision: ", precision_score(y_test, pred, average="weighted"))
print("Recall: ", recall_score(y_test, pred, average="weighted"))
print("F1: ", f1_score(y_test, pred, average="weighted"))

#Confusion Matrix plot
conf_mat = confusion_matrix(y_test, pred)
plt.matshow(conf_mat, cmap= plt.cm.rainbow)
plt.title("Confusion Matrix")
plt.colorbar()
plt.show()

"""The performance of the training set with 5 rows has a accuracy of 0.6. When we increase the training set the accuracy increases to 0.68. This shows that the tree is not a suitable model for this problem.

But we will look into the accuracy of testing set of the decision tree is 0.68, the precision is 0.56, the recall is 0.68 and the f1 score is 0.62 at a "max_depth" of two.

Increasing the "max_depth" from two to four has not yeilded an big increase of performance. The only thing that the max_depth has done is dispay the "other" class in the tree plot.

The confusion matrix plot has made no correct decision on the "dance pop" category. The tree diagram shows no "other" genre with a depth of two.

"""

#import tree
from sklearn import tree

#plot the decision treee
plt.figure(figsize=(10,5))
tree.plot_tree(tree_clf,class_names=y.unique(),filled=True)
plt.show()

#making sure we have three unique classes
print(y.unique())

"""# 6. Random Forests

Random forest is an ensamble of the bagging method in the decision tree [1]. This is a more convient way to optimise the decision tree [1]. Also note that the parameters are "n_estimators" which is set to  100, we have a "max_leaf_nodes" of 16 and "n_jobs" of -1.

The random forest has extra randomness when we are growing the tree compared to the decision tree [1].
This also trades a higher bias for a lower variance [1].

We will also use a feature importance to measure the importance of each features.
"""

#import the random forest classifier
from sklearn.ensemble import RandomForestClassifier

#call the random forest with its parameters
forest_clf = RandomForestClassifier(n_estimators=100,max_leaf_nodes=16 ,
                                    n_jobs=-1, random_state=24)
#fit the random forest
forest_clf.fit(X_train, y_train)


#Evaluation with the Training set with 5 rows
print("Accuracy of training set with 5:",
      accuracy_score(y_train[:5],forest_clf.predict(X_train[:5])))

#Evaluation with the Training set
print("Accuracy of training set:",
      accuracy_score(y_train,forest_clf.predict(X_train)))

#predict using the X_test
pred = forest_clf.predict(X_test)

#Evaluate the performace of the predicted and the y_test
print("Confusion Matrix: ", "\n", confusion_matrix(y_test, pred))
print("Accuracy: ", accuracy_score(y_test, pred))
print("Precision: ", precision_score(y_test, pred, average="weighted"))
print("Recall: ", recall_score(y_test, pred, average="weighted"))
print("F1: ", f1_score(y_test, pred, average="weighted"))

#Confusion Matrix plot
conf_mat = confusion_matrix(y_test, pred)
plt.matshow(conf_mat, cmap= plt.cm.rainbow)
plt.title("Confusion Matrix")
plt.colorbar()
plt.show()

#find the features and their importance
for name, score in zip(forest_clf.feature_names_in_,
                       forest_clf.feature_importances_):
    print(name, score)

"""Suprisingly, the accuracy of the training set was perfect with 5 rows but decreased as we increased the sample size to 0.78. This shows that of the previous three model, this classification performed the best.

The accuracy of the model in the testing set is 0.72, the precision is 0.59, the recall is 0.72 and the f1-score is 0.64. This has performed better than the decision tree. But again it has failed to correctly identify the "dance pop" genre just like the decision tree.

The random forest and OVR logistic model have the same performance.

Looking at the importance feacture we see that "nrgy" has the most importance with a value of 0.16 and "log_live" has the least importance with a value of 0.052.

# 7. Ensamble Learning: Voting Classifiers

We will train the all the classifiers such as the logisitic regression, OVR logisitic regression, decision tree and random forest and create a voting classifer using the hard voting parameter.
"""

#import the ensamble voting classifier
from sklearn.ensemble import VotingClassifier

#logistic regression
log_reg = LogisticRegression(random_state=24)

#OVR logisitic regression
ovr_log_reg = OneVsRestClassifier(LogisticRegression(random_state=24))
#random forest classifier
forest_clf = RandomForestClassifier(n_estimators=100,max_leaf_nodes=16,
                                    n_jobs=-1, random_state=24)
#decision tree classifier
tree_clf = DecisionTreeClassifier(max_depth=2, random_state=24)

#create the voting classifier
voting_clf = VotingClassifier(estimators=[('lr', log_reg),
                                           ('ovr_log_reg', ovr_log_reg),
                                           ('forest_clf', forest_clf),
                                           ('tree_clf', tree_clf)],
                              voting='hard')
#fit the voting classifier
voting_clf.fit(X_train, y_train)

#predict each classifier
for clf in (log_reg,ovr_log_reg,forest_clf, tree_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, "Confusion matrix" , "\n",
          confusion_matrix(y_test, y_pred))
    print(clf.__class__.__name__, "Accuracy :",
          accuracy_score(y_test, y_pred))
    print(clf.__class__.__name__, "Precision :",
          precision_score(y_test, y_pred, average="weighted"))
    print(clf.__class__.__name__, "Recall :",
          recall_score(y_test, y_pred, average="weighted"))
    print(clf.__class__.__name__, "F1 :",
          f1_score(y_test, y_pred, average="weighted"))

"""Using all the models above such as the simple Logisitic regression, OVR logisitc regression, decision tree and the random forest model.

The voting classifier model has an accuracy of 0.72, a precison of 0.68, recall of 0.72 and a f1 score of 0.66 which is the best score we have produced so far. This means that ensamble learning has produced the best classifier model.

The confusion matrix aslo shows that the "dance pop" genre has seen three correctly predicted.

# 9. Discussion

The main aspect of this report is the changing the 115 genres in "top genre" in to three. We first worked with the 115 genres and found that the performance of all the models were 30% or below. After making three groups, the ratio of the three groups of "other", "dance pop" and "adult standards" is 0.68, 0.18 and 0.14 respectively.

The training set performance of the simple and multiclass logisitic regression models both had high accuracy when we had a low number of training set but decreased. This could be a sign of overfitting because of the genre groups being unbalanced.
The decision tree had a low performance then increased, this showed some underfitting but slowly as we increase the training set the accuracy increase.
The random forest had a same trend as the logisitc regression but the accuracy of the full training set was the highest.

The random forest has an important feature hyperparameter, using the information, we dropped any feature that was below 0.1 score. Leading to 5 features being dropped.
We ran the model again without the features but had the same performance result. The only difference was that the importance of the remaining features increased, this was due to the reduction of the number of features present.

Our model was heavily affected by the preprecessing, having a target data of the "top genres" which was unbalanced both before and after changes, which caused our model to not have high performance measures because of the overfitting. A suggestion would be to group different genres in a major genre, for example glam rock and album rock would be under the the major genre "rock".

The models were chosen because of their application and performance in the training set. But ultimately the best was the ensamble methods worked the best.

A suggestion of imporvement would be dropping more features which are statisitically not significant, using other visualizations such as boxplots. Grouping genres together and create a balanced genres in "top genre".

# 10: Conclusion

In conclusion, the Ensamble method had the best F1 score which is used to compare different classification model [1]. This means to predict the "top genre" ("adult standards", "dance pop" or "other") that a song belongs to given that we have the 10 features provided ("bpm", "nrgy", "dnce", "dB", "val", "dur", "pop", "log_acous", "log_spch" and "log_live"). We can accuarately predict it by 72%.
Other than the voting classifer the random forrest also performed well in both the training and testing set.

This is shown below where we have a new X variable which can represent the features of a new song. And the classification is the "other" genre.
"""

# A new song with the its 10 features
#X.columns
X_new = [92,80,66,-11,79,123,62,4.5,2.9,4.2]
print(voting_clf.predict([X_new]))
#print(forest_clf.predict([X_new]))

"""# References:

[1] Geron, A.G, 2019, *Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow*, Last Accessed 8 June 2024
"""